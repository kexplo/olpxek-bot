import asyncio

from discord.ext import commands
from openai import AsyncOpenAI
from rich import print


client = AsyncOpenAI(
    api_key="dummy",
    base_url="http://146.56.128.82:9997/v1",
)


async def request_chat_stream(prompt: str):
    # LLaMA Prompt:
    # full_prompt = "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n"  # noqa: E501
    # full_prompt += f"### Instructions: \n{prompt}\n\n### Response: \n"
    # 42dot LLM Prompt:
    # full_prompt = "í˜¸ê¸°ì‹¬ ë§ì€ ì¸ê°„ (human)ê³¼ ì¸ê³µì§€ëŠ¥ ë´‡ (AI bot)ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. ë´‡ì€ ì¸ê°„ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. "  # noqa: E501
    # full_prompt += f"<human>: {prompt} <bot>:"

    final_response_text = ""
    stream = await client.chat.completions.create(
        model="gemma-2b-it-gguf",
        messages=[{"role": "user", "content": prompt}],
        stream=True,
    )
    async for chunk in stream:
        chunk_text = chunk.choices[0].delta.content or ""
        if chunk_text:
            final_response_text += chunk_text
            yield final_response_text


class LLMCog(commands.Cog):
    def __init__(self):
        self.llama_cmd_lock = asyncio.Lock()

    @commands.command(aliases=("ì•ŒíŒŒì¹´", "alpaca", "llama"))
    async def llm(self, ctx, *, arg):
        print(f"LLM request: {arg}")
        async with self.llama_cmd_lock:
            await ctx.message.add_reaction("ğŸ’¬")
            message = await ctx.reply("ğŸ’¬..")
            content = ""

            async def recv_llama_stream(prompt: str):
                nonlocal content
                async for content in request_chat_stream(arg):
                    # request_chat_stream returns accumulated content
                    content = content

            try:
                task = asyncio.create_task(recv_llama_stream(arg))
                prev_content = ""
                while not task.done():
                    if not content.strip():
                        await asyncio.sleep(0.1)
                        continue
                    if prev_content == content:
                        # content not changed
                        await asyncio.sleep(0.1)
                        continue
                    prev_content = content
                    await message.edit(content=content)
                    await asyncio.sleep(1)  # prevent rate limit
                task.result()  # raise exception if any
            except Exception as e:
                await ctx.message.add_reaction("âŒ")
                print(e)
                prev_content = message.content
                if prev_content:
                    new_content = prev_content + "\n---\n" + str(e)
                else:
                    new_content = str(e)
                await message.edit(content=new_content)
                return
            await ctx.message.add_reaction("âœ…")
            return


if __name__ == "__main__":

    async def test_main():
        async for content in request_chat_stream("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?"):
            print(content)

    asyncio.run(test_main())
